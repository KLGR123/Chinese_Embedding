{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07deffa0",
   "metadata": {},
   "source": [
    "# 汉语子词向量任务 SGNS\n",
    "\n",
    "本实验基于 SGNS 方法构建汉语子词向量并进行评测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26dab7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c5798a",
   "metadata": {},
   "source": [
    "首先导入 corpus 语料库，并得到句子序列 sentences，查看语料库总共有 260004 条。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad9e14db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    lines = []\n",
    "    with open(file_name, 'r') as f: \n",
    "        for line in f:\n",
    "            lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecc0d4d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = read_file('src/corpus.txt')\n",
    "sentences = sentences[:100000]\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e401ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 句子预处理\n",
    "对 sentences 序列中的每个句子元素进行预处理，包括如下步骤。\n",
    "- 除去数据中空格（包括非文本、标点等）\n",
    "- 除去停用词\n",
    "- 中文分词\n",
    "\n",
    "使用这些方法可以得到处理后的 sentences。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d97a3587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def purify(line): \n",
    "    line = line.replace(' ', '')\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20d37c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.685 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['上涨', '最后', '一样', '继续', '美国']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "from jieba import analyse\n",
    "\n",
    "analyse.set_stop_words('src/stopwords.txt') # 停用词表设定\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = purify(sentences[i])\n",
    "    sentences[i] = analyse.extract_tags(sentences[i])\n",
    "\n",
    "sentences[0][-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0503f916",
   "metadata": {},
   "source": [
    "### 生成词表\n",
    "\n",
    "通过对 corpus.txt 中的所有语句进行分词，得到词表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7206025d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132474"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = set([]) # 词表集合\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences[i])):\n",
    "        words.add(sentences[i][j])\n",
    "\n",
    "words = list(words)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce37109",
   "metadata": {},
   "source": [
    "从中随机取出 10000 个词，最终得到词表 words。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bea6f041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, '爱子')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def get_word_pair(line):\n",
    "    line = line.strip(\"\\n\")\n",
    "    return line.split()\n",
    "     \n",
    "words = random.sample(words, 9000) \n",
    "\n",
    "with open('src/word_pair.txt', 'r') as f: # 加入测试词汇\n",
    "    for line in f:\n",
    "        worda, wordb = get_word_pair(line)\n",
    "        words.append(worda)\n",
    "        words.append(wordb) \n",
    "        \n",
    "len(words), words[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0997243-6047-47ca-be46-9d99c4897b7f",
   "metadata": {},
   "source": [
    "将句子根据词表 token 取值替代，换成 one-hot 形式便于生成训练数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c3bfc86-8941-4c66-beb8-a0a4136bdb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [06:58<00:00, 238.83it/s]\n"
     ]
    }
   ],
   "source": [
    "for sentence in tqdm.tqdm(sentences):\n",
    "    for j in range(len(sentence)):\n",
    "        if sentence[j] in words:\n",
    "            sentence[j] = words.index(sentence[j])\n",
    "        else:\n",
    "            sentence[j] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c4f60f",
   "metadata": {},
   "source": [
    "### 基于 SGNS 分解\n",
    "\n",
    "跳元模型假设一个词可以用来在文本序列中生成周围的词，跳元模型的主要思想是使用 softmax 运算来计算基于给定的中心词生成上下文字的条件概率。\n",
    "\n",
    "跳元模型的梯度计算包含求和。然而，在一个词典上求和的梯度的计算成本是很大的。为了降低计算复杂度，使用负采样技术。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570a9297",
   "metadata": {},
   "source": [
    "#### 1 构造数据生成器\n",
    "中心词和上下文词的提取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "347be93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centers_and_contexts(sentences, window_size): # 返回跳元模型中的中心词和上下文词\n",
    "    centers, contexts = [], []\n",
    "    \n",
    "    for line in tqdm.tqdm(sentences):\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "            \n",
    "        centers += line\n",
    "        \n",
    "        for i in range(len(line)):\n",
    "            indices = list(range(max(0, i - window_size), min(len(line), i + 1 + window_size)))\n",
    "            indices.remove(i) # 排除中心词\n",
    "            \n",
    "            contexts.append([line[idx] for idx in indices])\n",
    "                \n",
    "    return centers, contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622682de",
   "metadata": {},
   "source": [
    "使用负采样进行近似训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "997c0371",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomGenerator: # 根据 n 个采样权重在 1 到 n 中随机抽取    \n",
    "    def __init__(self, sampling_weights):\n",
    "        self.population = list(range(1, len(sampling_weights) + 1))\n",
    "        self.sampling_weights = sampling_weights\n",
    "        self.candidates = []\n",
    "        self.i = 0\n",
    "\n",
    "    def draw(self):\n",
    "        if self.i == len(self.candidates):\n",
    "            self.candidates = random.choices(self.population, self.sampling_weights, k=10000)\n",
    "            self.i = 0\n",
    "            \n",
    "        self.i += 1\n",
    "        \n",
    "        return self.candidates[self.i - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36b2b949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negatives(all_contexts, words, n): # 返回负采样中的噪声词\n",
    "    sampling_weights = [1 ** 0.75 for i in range(1, len(words))] # 由于词表数量有限，采取等权重方案\n",
    "    all_negatives, generator = [], RandomGenerator(sampling_weights)\n",
    "    \n",
    "    for contexts in tqdm.tqdm(all_contexts):\n",
    "        negatives = []\n",
    "        \n",
    "        while len(negatives) < len(contexts) * n:\n",
    "            neg = generator.draw()\n",
    "            if neg not in contexts: # 噪声词不能是上下文词\n",
    "                negatives.append(neg)\n",
    "                \n",
    "        all_negatives.append(negatives)\n",
    "        \n",
    "    return all_negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86685a99",
   "metadata": {},
   "source": [
    "在提取所有中心词及其上下文词和采样噪声词后，将它们转换成小批量的样本，在训练过程中可以迭代加载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "280251a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data): # 返回带有负采样的跳元模型的小批量样本  \n",
    "    max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "    centers, contexts_negatives, masks, labels = [], [], [], []\n",
    "    \n",
    "    for center, context, negative in data:\n",
    "        cur_len = len(context) + len(negative)\n",
    "        \n",
    "        centers += [center]\n",
    "        contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "        masks += [[1] * cur_len + [0] * (max_len - cur_len)] # 构建掩码\n",
    "        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
    "        \n",
    "    return (torch.tensor(centers).reshape((-1, 1)), torch.tensor(contexts_negatives), \n",
    "            torch.tensor(masks), torch.tensor(labels)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c24ce0b",
   "metadata": {},
   "source": [
    "综合以上函数，构造数据生成器，以便训练时调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afd54904",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset): # 定义数据集\n",
    "    def __init__(self, centers, contexts, negatives):\n",
    "        assert len(centers) == len(contexts) == len(negatives)\n",
    "        self.centers = centers\n",
    "        self.contexts = contexts\n",
    "        self.negatives = negatives\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.centers[index], self.contexts[index], self.negatives[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a9faee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size, window_size, num_noise_words, words, sentences): # 加载数据\n",
    "    all_centers, all_contexts = get_centers_and_contexts(sentences, window_size)     \n",
    "    all_negatives = get_negatives(all_contexts, words, num_noise_words)\n",
    "    dataset = Dataset(all_centers, all_contexts, all_negatives)\n",
    "\n",
    "    data_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True, collate_fn=batchify)\n",
    "    return data_iter, words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4c61f9",
   "metadata": {},
   "source": [
    "#### 2 加载数据迭代器\n",
    "设定训练批次大小，窗口 K 值，负采样个数。生成数据迭代器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95db713d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:02<00:00, 37301.28it/s]\n",
      "100%|██████████| 1474330/1474330 [00:16<00:00, 90052.05it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size, window_size, num_noise_words = 256, 2, 3\n",
    "data_iter, words = load_data(batch_size, window_size, num_noise_words, words, sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ad1b0f",
   "metadata": {},
   "source": [
    "#### 3 定义前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12c46f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n",
    "    if (-1 not in center) and (-1 not in contexts_and_negatives):\n",
    "        v = embed_v(center)\n",
    "        u = embed_u(contexts_and_negatives)\n",
    "        pred = torch.bmm(v, u.permute(0, 2, 1))\n",
    "        return pred\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae63ef5a",
   "metadata": {},
   "source": [
    "#### 4 定义二元交叉熵损失函数\n",
    "定义带掩码的训练损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2e734ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidBCELoss(nn.Module): # 带掩码的二元交叉熵损失\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs, target, mask=None):\n",
    "        out = nn.functional.binary_cross_entropy_with_logits(\n",
    "            inputs, target, weight=mask, reduction=\"none\")\n",
    "        return out.mean(dim=1)\n",
    "\n",
    "loss = SigmoidBCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ed29e3",
   "metadata": {},
   "source": [
    "#### 5 模型初始化\n",
    "定义了两个嵌入层，将词表中的所有单词分别作为中心词和上下文词使用。字向量维度 embed_size 设为 200。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "562cadb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 200\n",
    "net = nn.Sequential(nn.Embedding(num_embeddings=len(words), embedding_dim=embed_size),\n",
    "                    nn.Embedding(num_embeddings=len(words), embedding_dim=embed_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ea8312",
   "metadata": {},
   "source": [
    "#### 6 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "52baa15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data_iter, lr, num_epochs):\n",
    "    def init_weights(m): # 初始词向量\n",
    "        if type(m) == nn.Embedding:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    net.apply(init_weights)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "        for i, batch in enumerate(data_iter):\n",
    "            optimizer.zero_grad()\n",
    "            center, context_negative, mask, label = [data for data in batch]\n",
    "            pred = skip_gram(center, context_negative, net[0], net[1]) # 前向预测\n",
    "            \n",
    "            if pred != None:\n",
    "                l = (loss(pred.reshape(label.shape).float(), label.float(), mask)\n",
    "                         / mask.sum(axis=1) * mask.shape[1])\n",
    "\n",
    "                l.sum().backward() # 反向传播\n",
    "                optimizer.step()\n",
    "\n",
    "    return net[0].weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150d7126",
   "metadata": {},
   "source": [
    "调参学习率为 0.002，训练轮数为5。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "42848db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:43<00:00,  8.69s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 200])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr, num_epochs = 0.002, 5\n",
    "V = train(net, data_iter, lr, num_epochs)\n",
    "V.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c25042",
   "metadata": {},
   "source": [
    "#### 相似度验证 SGNS 分解\n",
    "与 SVD 实现相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a8ab6c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(worda, wordb, V): # 计算两个词的余弦相似度\n",
    "    if (worda in words) and (wordb in words):\n",
    "        a = words.index(worda)\n",
    "        b = words.index(wordb)\n",
    "        cos = np.dot(V[a], V[b]) / (np.linalg.norm(V[a]) * np.linalg.norm(V[b]))\n",
    "        return cos\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "57ef193e-e1b4-4c9a-b7d7-685d6ab0d68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "lines = ''\n",
    "\n",
    "with open('src/test.txt', 'r') as f: \n",
    "    for line in f:\n",
    "        worda, wordb, svd_sim = get_word_pair(line)\n",
    "        line = line[:-1] + '\\t' + str(cos_sim(worda, wordb, V)) + '\\n'\n",
    "        lines += line\n",
    "    \n",
    "with open('src/test.txt', 'w') as f: \n",
    "    f.write(lines)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.8",
   "language": "python",
   "name": "torch1.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
